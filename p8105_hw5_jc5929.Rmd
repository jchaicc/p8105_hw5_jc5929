---
title: "p8105_hw5_jc5929"
output: github_document
date: "2022-11-13"
---

```{r}
library(tidyverse)
library(knitr)
library(purrr)
```

```{r}
hom_df=read_csv("data/homicide-data.csv") %>%
  mutate(city_state=str_c(city,", ", state)) %>%
  filter(city_state != "Tulsa, AL")
```

__Description on the raw dataset:__
This dataset contains homicides data in 50 cities in the U.S. There were 12 variable describing the characteristics of each victim in the raw data, including their name, age, sex, and the disposition status. There were 52179 entries in total.

## Problem 2
```{r}
new_hom_data= hom_df %>%
  mutate(status=case_when(disposition=="Closed without arrest" ~ "unsolved",
                          disposition=="Open/No arrest" ~ "unsolved",
                          disposition=="Closed by arrest" ~ "solved")) %>%
  select(city_state,status) %>%
  group_by(city_state) %>%
  summarize(total_hom=n(),
            unsolve_obs=sum(status=="unsolved")) 
new_hom_data

```

```{r}
bm_test=prop.test(new_hom_data%>%filter(city_state=="Baltimore, MD")%>%pull(unsolve_obs),
          new_hom_data%>% filter(city_state=="Baltimore, MD")%>%pull(total_hom)) %>%
  broom::tidy() 

bm_est=bm_test%>%pull(estimate) 
bm_est
bm_conf_low=bm_test%>%pull(conf.low)
bm_conf_low
bm_conf_high=bm_test%>%pull(conf.high)
bm_conf_high
  
```

```{r}
prop_test= new_hom_data %>%
  mutate(output=map2(.x = unsolve_obs, .y = total_hom, ~prop.test(x=.x,n=.y)),
         tidy=map(.x=output,~broom::tidy(.x)))%>%
  select(-output) %>%
  unnest(tidy) %>%
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
prop_test %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x=city_state,y=estimate)) +geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Est. Proportion of Unsolved Homicides"
  )

```

## Problem 3

```{r}
sim_mean = function(n=30, mu=0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  t_test=t.test(sim_data,mean=mu)
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      p_value= pull(broom::tidy(t_test),p.value)
    )
}

```

```{r}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_mean(mu=0)
}

sim_results = bind_rows(output)
sim_results

```

```{r}
sim_results_df = 
  tibble(sample_mu = c(1, 2, 3, 4,5,6)) %>% 
  mutate(
    output_lists = map(.x = sample_mu, ~rerun(5000, sim_mean(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

```{r}
sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject"))%>%
  mutate(as.factor(sample_mu)) %>%
  group_by(sample_mu)%>%
  summarize(total=n(),
            reject_obs=sum(decision=="reject"),
            reject_prop=reject_obs/total) %>%
  ggplot(aes(x=sample_mu,y=reject_prop))+geom_point()+geom_line()+
  labs(title = "Proportion of null hypothesis rejection for each true mean")
```
According to the plot, power is positively related to effect size. The `effect size = (true mean - mu null)` will increase since the true mu increase from 1 to 6, and mu null also maintained to be 0. Thus, the power will increased and reached the highest value 1 when mu is 4.
```{r}
reject_df=
  sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject")) %>%
  filter(decision=="reject") %>%
  group_by(sample_mu) %>%
  summarize(ave_mu=mean(mu_hat))



  total_samp=sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject"))%>%
  mutate(as.factor(sample_mu)) %>%
  group_by(sample_mu)%>%
  summarize(ave_mu=mean(mu_hat)) 
  
  combine_df=bind_rows(list(a = total_samp, b = reject_df), .id = 'group')
  
  ggplot(combine_df)+
  geom_line(aes(x = sample_mu, y = ave_mu, color = group))+
  scale_color_manual(values = c("blue", "red"),
    labels = c(expression("whole sample"), expression("sample for which null is rejected")))+
    labs(x = "true value of µ", y = "average estimate of µ-hat",
       title = "Average estimate of µ-hat vs. True value of µ")



```

When µ is smaller than 3, the average of µ-hat for which the sample where null is rejected is higher than  true value of µ. But µ gets larger, the average of µ-hat across tests where the null is rejected approaches the true value of µ.

 we can see the power is relatively low when µ is smaller than 3, which means the proportion of times the null was rejected is low. Therefore, for µ smaller than 3, the number of samples with rejected null is relatively small. These sample means indeed have more variation and their average is more likely to deviate from the true mean. However, as µ gets bigger, there are more samples with rejected null, so the sample average of µ-hat becomes more stable and yields a more unbiased estimation of the true mean.

 
