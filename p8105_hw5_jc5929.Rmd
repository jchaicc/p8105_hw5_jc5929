---
title: "p8105_hw5_jc5929"
output: github_document
date: "2022-11-13"
---

```{r}
library(tidyverse)
library(knitr)
library(purrr)
```

## Problem 1

```{r}

knitr::opts_chunk$set(
  message = FALSE)
```

```

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 


## Problem 2

```{r}
hom_df=read_csv("data/homicide-data.csv") %>%
  mutate(city_state=str_c(city,", ", state)) %>%
  filter(city_state != "Tulsa, AL")
```

__Description on the raw dataset:__
This dataset contains homicides data in 50 cities in the U.S. There were 12 variable describing the characteristics of each victim in the raw data, including their name, age, sex, and the disposition status. There were 52179 entries in total.

```{r}
new_hom_data= hom_df %>%
  mutate(status=case_when(disposition=="Closed without arrest" ~ "unsolved",
                          disposition=="Open/No arrest" ~ "unsolved",
                          disposition=="Closed by arrest" ~ "solved")) %>%
  select(city_state,status) %>%
  group_by(city_state) %>%
  summarize(total_hom=n(),
            unsolve_obs=sum(status=="unsolved")) 
new_hom_data

```

```{r}
prop.test(new_hom_data%>%filter(city_state=="Baltimore, MD")%>%pull(unsolve_obs),
          new_hom_data%>% filter(city_state=="Baltimore, MD")%>%pull(total_hom)) %>%
  broom::tidy()  %>%
   select(estimate, conf.low, conf.high) %>%
  knitr::kable()

  
```

```{r}
prop_test= new_hom_data %>%
  mutate(output=map2(.x = unsolve_obs, .y = total_hom, ~prop.test(x=.x,n=.y)),
         tidy=map(.x=output,~broom::tidy(.x)))%>%
  select(-output) %>%
  unnest(tidy) %>%
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
prop_test %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x=city_state,y=estimate)) +geom_point()+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Est. Proportion of Unsolved Homicides"
  )

```

## Problem 3

```{r}
sim_mean = function(n=30, mu=0, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  t_test=t.test(sim_data,mean=mu)
  sim_data %>% 
    summarize(
      mu_hat = mean(x),
      p_value= pull(broom::tidy(t_test),p.value)
    )
}

```

```{r}
output = vector("list", 5000)

for (i in 1:5000) {
  output[[i]] = sim_mean(mu=0)
}

sim_results = bind_rows(output)
sim_results

```

```{r}
sim_results_df = 
  tibble(sample_mu = c(1, 2, 3, 4,5,6)) %>% 
  mutate(
    output_lists = map(.x = sample_mu, ~rerun(5000, sim_mean(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

```{r}
sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject"))%>%
  mutate(as.factor(sample_mu)) %>%
  group_by(sample_mu)%>%
  summarize(total=n(),
            reject_obs=sum(decision=="reject"),
            reject_prop=reject_obs/total) %>%
  ggplot(aes(x=sample_mu,y=reject_prop))+geom_point()+geom_line()+
  labs(title = "Proportion of null hypothesis rejection for each true mean")
```
According to the plot, power is positively related to effect size. The `effect size = (true mean - mu null)` will increase since the true mu increase from 1 to 6, and mu null also maintained to be 0. Thus, the power will increased and reached the highest value 1 when mu is 4.
```{r}
reject_df=
  sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject")) %>%
  filter(decision=="reject") %>%
  group_by(sample_mu) %>%
  summarize(ave_mu=mean(mu_hat))



  total_samp=sim_results_df %>%
  mutate(decision= case_when( p_value<0.05 ~ "reject",
                              p_value>0.05 ~ "fail to reject"))%>%
  mutate(as.factor(sample_mu)) %>%
  group_by(sample_mu)%>%
  summarize(ave_mu=mean(mu_hat)) 
  
  combine_df=bind_rows(list(a = total_samp, b = reject_df), .id = 'group')
  
  ggplot(combine_df)+
  geom_line(aes(x = sample_mu, y = ave_mu, color = group))+
  scale_color_manual(values = c("blue", "red"),
    labels = c(expression("whole sample"), expression("sample for which null is rejected")))+
    labs(x = "true value of µ", y = "average estimate of µ-hat",
       title = "Average estimate of µ-hat vs. True value of µ")



```

When µ is smaller than 3, the average of µ-hat for which the sample where null is rejected is higher than  true value of µ. But µ gets larger, the average of µ-hat across tests where the null is rejected approaches the true value of µ.

From the previous plot, we can see that the proportion of times the null was rejected is low. So for µ smaller than 3, the number of samples with rejected null is relatively small. These sample have more variation and their average is more likely to deviate from the true mean. However, as µ gets bigger, there are more samples that rejects null, so the sample average of µ-hat becomes more stable and yields a more unbiased estimation of the true mean.

 
